{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "RecommendationSystem.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkXTlOM142FH",
        "colab_type": "text"
      },
      "source": [
        "# SGD Algorithm to predict movie ratings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pveXiAD42FJ",
        "colab_type": "text"
      },
      "source": [
        "<pre>\n",
        "1. Download the data from <a href='https://drive.google.com/open?id=1-1z7iDB52cB6_JpO7Dqa-eOYSs-mivpq'> here </a>\n",
        "2. the data will be of this formate, each data point is represented as a triplet of user_id, movie_id and rating \n",
        "<table>\n",
        "<tr><th>user_id</th><th>movie_id</th><th>rating</th></tr>\n",
        "<tr><td>77</td><td>236</td><td>3</td></tr>\n",
        "<tr><td>471</td><td>208</td><td>5</td></tr>\n",
        "<tr><td>641</td><td>401</td><td>4</td></tr>\n",
        "<tr><td>31</td><td>298</td><td>4</td></tr>\n",
        "<tr><td>58</td><td>504</td><td>5</td></tr>\n",
        "<tr><td>235</td><td>727</td><td>5</td></tr>\n",
        "</table>\n",
        "<h3>task 1: Predict the rating for a given (user_id, movie_id) pair</h3>\n",
        "</pre>\n",
        "<ul>\n",
        "<li><span class=\"math\">\\(\\mu\\)</span> : scalar mean rating</li>\n",
        "<li><span class=\"math\">\\(b_i\\)</span> : scalar bias term for user <span class=\"math\">\\(i\\)</span></li>\n",
        "<li><span class=\"math\">\\(c_j\\)</span> : scalar bias term for movie <span class=\"math\">\\(j\\)</span></li>\n",
        "<li><span class=\"math\">\\(u_i\\)</span> : K-dimensional vector for user <span class=\"math\">\\(i\\)</span></li>\n",
        "<li><span class=\"math\">\\(v_j\\)</span> : K-dimensional vector for movie <span class=\"math\">\\(j\\)</span></li>\n",
        "</ul>\n",
        "then the predicted rating $\\hat{y}_{ij}$ for user i, movied j pair is calcuated as $\\hat{y}_{ij} = \\mu + b_i + c_j + u_i^T v_j$ here we will be finding the best values of $b_{i}$ and $c_{j}$ using SGD algorithm with the optimization problem for N users and M movies is defined as\n",
        "\n",
        "\n",
        "$$\n",
        "L = \\min_{ b, c, \\{ u_i \\}_{i=1}^N, \\{ v_j \\}_{j=1}^M}\n",
        "\\quad\n",
        "\\alpha \\Big(\n",
        "    \\sum_{j} \\sum_{k} v_{jk}^2 \n",
        "    + \\sum_{i} \\sum_{k} u_{ik}^2 \n",
        "    + \\sum_{i} b_i^2\n",
        "    + \\sum_{j} c_i^2\n",
        "    \\Big)\n",
        "+ \\sum_{i,j \\in \\mathcal{I}^{\\text{train}}}\n",
        "    (y_{ij} - \\mu - b_i - c_j - u_i^T v_j)^2\n",
        "$$\n",
        "\n",
        "### TASK: 1\n",
        "__SGD Algorithm to minimize the loss__\n",
        "1. for each unique user initilize a bias value $B_{i}$ randomly, so if we have $N$ users $B$ will be a $N$ dimensional vector, the $i^{th}$ value of the $B$ will corresponds to the bias term for $i^{th}$ user\n",
        "\n",
        "2. for each unique movie initilize a bias value $C_{j}$ randomly, so if we have $M$ movies $C$ will be a $M$ dimensional vector, the $j^{th}$ value of the $C$ will corresponds to the bias term for $j^{th}$ movie\n",
        "\n",
        "3. Construct adjacency matrix with the given data, assumeing its  <a href='https://en.wikipedia.org/wiki/Bipartite_graph'> weighted un-directed bi-partited graph</a> and the weight of each edge is the rating given by user to the movie\n",
        "<img src='https://i.imgur.com/rmUCGMb.jpg' width=200>\n",
        "you can construct this matrix like $A[i][j]=r_{ij}$ here $i$ is user_id, $j$ is movie_id and $r_{ij}$ is rating given by user $i$ to the movie $j$\n",
        "\n",
        "4. we will Apply SVD decomposition on the Adjaceny matrix <a href='https://stackoverflow.com/a/31528944/4084039'>link1</a>, <a href='https://machinelearningmastery.com/singular-value-decomposition-for-machine-learning/'> link2</a> and get three matrices $U, \\sum, V$ such that $U \\times \\sum \\times V^T = A$, <br> \n",
        "if $A$ is of dimensions $N \\times M$ then <br>\n",
        "U is of $N \\times k$, <br>\n",
        "$\\sum$ is of $k \\times k$ and <br>\n",
        "$V$ is $M \\times k$ dimensions. <br>\n",
        "\n",
        "5. So the matrix $U$ can be represented as matrix representation of users, where each row $u_{i}$ represents a k-dimensional vector for a user\n",
        "\n",
        "6. So the matrix $V$ can be represented as matrix representation of movies, where each row $v_{j}$ represents a k-dimensional vector for a movie\n",
        "\n",
        "7. $\\mu$ represents the mean of all the rating given in the dataset\n",
        "</pre>\n",
        "\n",
        "<br>8.\n",
        "<pre>\n",
        "for each epoch:\n",
        "    for each pair of (user, movie):\n",
        "        b_i =  b_i - learning_rate * dL/db_i\n",
        "        c_j =  c_j - learning_rate * dL/dc_j\n",
        "    predict the ratings with formula</pre>$\\hat{y}_{ij} = \\mu + b_i + c_j + \\text{dot_product}(u_i , v_j) $\n",
        " <pre>\n",
        "    print the mean squared error with predicted ratings\n",
        "    </pre>\n",
        "\n",
        "9. you can choose any learning rate and regularization term in the range $10^{-3}  \\text{ to } 10^2$  <br>\n",
        "\n",
        "10. __bonus__: instead of using SVD decomposition you can learn the vectors $u_i$, $v_j$ with the help of SGD algo similar to $b_i$ and $c_j$ \n",
        "### TASK: 2\n",
        "\n",
        "As we know U is the learned matrix of user vectors, with its i-th row as the vector ui for user i. Each row of U can be seen as a \"feature vector\" for a particular user.\n",
        "\n",
        "The question we'd like to investigate is this: do our computed per-user features that are optimized for predicting movie ratings contain anything to do with gender?\n",
        "\n",
        "The provided data file <a href='https://drive.google.com/open?id=1PHFdJh_4gIPiLH5Q4UErH8GK71hTrzlY'>user_info.csv</a> contains an is_male column indicating which users in the dataset are male. Can you predict this signal given the features U?\n",
        "\n",
        "\n",
        "> __Note 1__ : there is no train test split in the data, the goal of this assignment is to give an intution about how to do matrix factorization with the help of SGD and application of truncated SVD. for better understanding of the collabarative fillerting please check netflix case study. <br><br>\n",
        "> __Note 2__ : Check if scaling of $U$, $V$ matrices improve the metric "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeZ8EeYEs0dT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpDkbFiBsoyB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install stellargraph"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEH6X48GsxBw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import networkx as nx\n",
        "from networkx.algorithms import bipartite\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import pandas as pd\n",
        "# you need to have tensorflow\n",
        "from tensorflow.keras  import utils \n",
        "from tensorflow.python.keras import utils\n",
        "\n",
        "from stellargraph.data import UniformRandomMetaPathWalk\n",
        "from stellargraph import StellarGraph"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZOobKK-iJeZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjVR9oVj4ZHG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRmfFyjb4dcr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "link='https://drive.google.com/open?id=1CUH5nemfo0HVcl2ZcBmqvetqYVNfVNCa'\n",
        "fluff, id = link.split('=')\n",
        "print (id)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GB_NZT542FL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('ratings_train.csv') \n",
        "data = pd.read_csv('ratings_train.csv')\n",
        "data.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXCt7KL98HLi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "user_bias=[]\n",
        "item_bias=[]\n",
        "import random\n",
        "Unique_users=data['user_id'].unique().tolist()\n",
        "print(len(Unique_users))\n",
        "Unique_item=data['item_id'].unique().tolist()\n",
        "print(len(Unique_item))\n",
        "#generating user bias randomly\n",
        "for i in range(len(Unique_users)):\n",
        "  user_bias.append( random.uniform(-1,1))\n",
        "#print(len(user_bias))\n",
        "#generating item bias randomly\n",
        "for i in range(1681):\n",
        "  item_bias.append( random.uniform(-1,1))\n",
        "#print(item_bias)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pmHoBR2bit2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0A2Shldu8oU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "B = nx.Graph()\n",
        "edges = [tuple(x) for x in data.values.tolist()]\n",
        "print(len(edges))\n",
        "B.add_nodes_from(data['user_id'].unique(), bipartite=0, label='user_id')\n",
        "print(len(data['item_id'].unique()))\n",
        "B.add_nodes_from(data['item_id'].unique(), bipartite=1, label='item_id')\n",
        "B.add_weighted_edges_from(edges,label='rating')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLe3qVkNtSco",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip uninstall networkx -y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--boMvT8tJ_R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install networkx==2.3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMsfijxNNc4L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "A = list(nx.connected_component_subgraphs(B))[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "010jiCxcseNK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.sparse import csr_matrix\n",
        "adj_matrix = csr_matrix((data['rating'].values, (data['user_id'].values, data['item_id'].values)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_lHKVD342FP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.utils.extmath import randomized_svd\n",
        "import numpy as np \n",
        "#matrix = np.random.random((20, 10))\n",
        "U, Sigma, VT = randomized_svd(adj_matrix, n_components=3,n_iter=3, random_state=None)\n",
        "print(U.shape)\n",
        "vt=VT.T\n",
        "print(Sigma.shape)\n",
        "print(VT.T.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wd_NEjNJ3Gg6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean=data[\"rating\"].mean()\n",
        "print(mean)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRDG0-l-aPK9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def diff_item(j,p,q):\n",
        "  sum=0\n",
        "  \n",
        "  e=data.iloc[j]\n",
        "  y=e.rating\n",
        "  g=-2*(-q+y-(U[index_user].transpose()@vt[index_item].transpose())-mean-p)\n",
        "  #sum=sum+g\n",
        "  m=((2*0.1*q)+g)\n",
        "  return m"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEa3n5mOZ55z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def diff_user(j,p,q):\n",
        "  sum=0\n",
        "  \n",
        "  s=data.iloc[j]\n",
        "  y=s.rating\n",
        "  g=-2*(-p+y-(U[index_user].transpose()@vt[index_item].transpose())-mean-q)\n",
        "\n",
        "  m=((2*0.1*p)+g)\n",
        "  return m\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LURk45R4pvM",
        "colab_type": "code",
        "outputId": "dae7b136-b8d5-4714-fbee-8007023e3aa6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "column=data.iloc[:,0] \n",
        "y_orginal=data['rating'].values\n",
        "mse=0\n",
        "from sklearn.metrics import mean_squared_error\n",
        "x=[]\n",
        "z=[]\n",
        "for i in range(15) :\n",
        "  y=[]\n",
        "  \n",
        "  m=x\n",
        "  n=z\n",
        "  x=[]\n",
        "  z=[]\n",
        "  \n",
        "  \n",
        "  \n",
        "  for j in range(len(column)):\n",
        "    f=data.iloc[j]\n",
        "    user_id=f.user_id\n",
        "    #print(user_id)\n",
        "    item_id=f.item_id\n",
        "    #print(item_id)\n",
        "    index_user=Unique_users.index(user_id)\n",
        "    #print(index_user)\n",
        "    index_item=Unique_item.index(item_id)\n",
        "    if i==0:\n",
        "      p=user_bias[index_user]\n",
        "      q=item_bias[index_item]\n",
        "      \n",
        "      \n",
        "    else:\n",
        "      p=m[j]\n",
        "      q=n[j]\n",
        "         \n",
        "    \n",
        "    b_i =  p -0.1* diff_user(j,p,q)\n",
        "    #print(b_i)\n",
        "    c_j =  q - 0.1* diff_item(j,p,q)\n",
        "    #print(q)\n",
        "    b=mean+b_i+c_j+(U[index_user].transpose()@vt[index_item].transpose())\n",
        "    #print(q)\n",
        "    x.append(b_i)\n",
        "    z.append(c_j)\n",
        "    \n",
        "    y.append(b)\n",
        "  mse=mean_squared_error(y_orginal, y)\n",
        "  \n",
        "  print(\"MSE at epoch {0} is {1}\".format(i,mse))\n",
        "  \n",
        "\n",
        "    "
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MSE at epoch 0 is 0.6717429429105763\n",
            "MSE at epoch 1 is 0.24405490894201565\n",
            "MSE at epoch 2 is 0.09338757899302173\n",
            "MSE at epoch 3 is 0.038763105511437475\n",
            "MSE at epoch 4 is 0.01810224209392177\n",
            "MSE at epoch 5 is 0.009826517128049246\n",
            "MSE at epoch 6 is 0.00627382515244199\n",
            "MSE at epoch 7 is 0.004632831475536819\n",
            "MSE at epoch 8 is 0.0038221977067656025\n",
            "MSE at epoch 9 is 0.0033995104793519827\n",
            "MSE at epoch 10 is 0.0031703242800426305\n",
            "MSE at epoch 11 is 0.003042769397310667\n",
            "MSE at epoch 12 is 0.00297059508049474\n",
            "MSE at epoch 13 is 0.002929342024844226\n",
            "MSE at epoch 14 is 0.0029056197999486827\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYV14HH-afmw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d66FSge_atc5",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yq8kaIziatdE",
        "colab": {}
      },
      "source": [
        "link='https://drive.google.com/open?id=1iB6i4g-9BimPE_FWvcAQTO00-g-spQSQ'\n",
        "fluff, id = link.split('=')\n",
        "print (id)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPwxxdELFp51",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('user_info.csv') \n",
        "data = pd.read_csv('user_info.csv')\n",
        "data.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RtR4Ypt7FqLZ",
        "colab": {}
      },
      "source": [
        "from scipy.sparse import csr_matrix\n",
        "adj_matrix = csr_matrix((data['is_male'].values, (data['user_id'].values,data['age'].values)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oaR6ug9WFqLg",
        "colab": {}
      },
      "source": [
        "from sklearn.utils.extmath import randomized_svd\n",
        "import numpy as np \n",
        "#matrix = np.random.random((20, 10))\n",
        "U, Sigma, VT = randomized_svd(adj_matrix, n_components=3,n_iter=3, random_state=None)\n",
        "print(U.shape)\n",
        "vt=VT.T\n",
        "print(Sigma.shape)\n",
        "print(VT.T.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Mfqfuw7yatdJ",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('user_info.csv') \n",
        "data = pd.read_csv('user_info.csv')\n",
        "data.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkGDFw06bDC8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "Y=data['is_male'].values\n",
        "X=data.drop(['is_male'],axis=1).values\n",
        "clf = LogisticRegression(random_state=0).fit(X, Y)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OkVg9nWhklI",
        "colab_type": "code",
        "outputId": "496afc37-08c6-4dd5-b8ea-917f36833e1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from scipy.stats import randint as sp_randint\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import math\n",
        "\n",
        "\n",
        "neigh =LogisticRegression(random_state=0)\n",
        "parameters = {'C': np.logspace(0, 4, 10),'penalty': ['l1', 'l2']}\n",
        "clf = RandomizedSearchCV(neigh, parameters,n_iter=7, cv=3, scoring='roc_auc')\n",
        "best_model=clf.fit(X, Y)\n",
        "\n",
        "print('Best Penalty:', best_model.best_estimator_.get_params()['penalty'])\n",
        "print('Best C:', best_model.best_estimator_.get_params()['C'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best Penalty: l1\n",
            "Best C: 7.742636826811269\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrITzALZ8rgA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "clf = LogisticRegression(random_state=0,penalty='l1', C=7.742636826811269,).fit(X, Y)\n",
        "Y_pred=clf.predict(U)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXR1g37O-v-w",
        "colab_type": "code",
        "outputId": "66a38021-cbb5-476e-a911-629912ce4878",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "print(Y_pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAfuHWRskOFV",
        "colab_type": "text"
      },
      "source": [
        "Above prediction using U vetor shows that features generated using SVD have no impact on prediction even after hyperparameter tuning i.e., SVD features, feature importance it as very low impact on prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rc15j2IJEdnm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}