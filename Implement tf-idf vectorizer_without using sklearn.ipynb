{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Implement tf-idf vectorizer_without using sklearn.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"h9464I-uxLiw"},"source":["# Assignment"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"dg2ooa4DxLiz"},"source":["## Task-1"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"OnV82tg1xLi0"},"source":["### Corpus"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"bUsYm9wjxLi1","colab":{}},"source":["## SkLearn# Collection of string documents\n","\n","corpus = [\n","     'this is the first document',\n","     'this document is the second document',\n","     'and this is the third one',\n","     'is this the first document',\n","]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"eLwmFZfKxLi4"},"source":["### SkLearn Implementation"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Np4dfQOkxLi4","colab":{}},"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","vectorizer = TfidfVectorizer()\n","vectorizer.fit_transform(corpus)\n","skl_output = vectorizer.transform(corpus)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"-7Om8YpYxLi6","outputId":"0a3bd0f5-4424-4400-944f-4482a80bd799","colab":{}},"source":["# sklearn feature names, they are sorted in alphabetic order by default.\n","\n","print(vectorizer.get_feature_names())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"dTKplK96xLi-","outputId":"53722fa2-6756-4aa0-f179-37b578bb6890","colab":{}},"source":["# Here we will print the sklearn tfidf vectorizer idf values after applying the fit method\n","# After using the fit function on the corpus the vocab has 9 words in it, and each has its idf value.\n","\n","print(vectorizer.idf_)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[1.91629073 1.22314355 1.51082562 1.         1.91629073 1.91629073\n"," 1.         1.91629073 1.        ]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"-CTiWHygxLjA","outputId":"8d5a9cde-2c29-4afe-f7b4-1547e88dba4f","colab":{}},"source":["# shape of sklearn tfidf vectorizer output after applying transform method.\n","\n","skl_output.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(4, 9)"]},"metadata":{"tags":[]},"execution_count":1280}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"bDKEpbA-xLjD","outputId":"87dafd65-5313-443f-8c6e-1b05cc8c2543","colab":{}},"source":["# sklearn tfidf values for first line of the above corpus.\n","# Here the output is a sparse matrix\n","\n","print(skl_output[0])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["  (0, 8)\t0.38408524091481483\n","  (0, 6)\t0.38408524091481483\n","  (0, 3)\t0.38408524091481483\n","  (0, 2)\t0.5802858236844359\n","  (0, 1)\t0.46979138557992045\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"3QWo34hexLjF","outputId":"cdc04e08-989f-4bdc-dd7f-f1c82a9f90be","colab":{}},"source":["# sklearn tfidf values for first line of the above corpus.\n","# To understand the output better, here we are converting the sparse output matrix to dense matrix and printing it.\n","# Notice that this output is normalized using L2 normalization. sklearn does this by default.\n","\n","print(skl_output[0].toarray())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n","  0.38408524 0.         0.38408524]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"qfIwx5LzxLjI"},"source":["### custom implementation"]},{"cell_type":"code","metadata":{"id":"I1TJ1DgSx3fo","colab_type":"code","colab":{}},"source":["def transform(dataset,vocab):\n","    rows = []\n","    columns = []\n","    values = []\n","    if isinstance(dataset, (list,)):\n","        for idx, row in enumerate(tqdm(dataset)): # for each document in the dataset\n","            # it will return a dict type object where key is the word and values is its frequency, {word:frequency}\n","            word_freq = dict(Counter(row.split()))\n","            # for every unique word in the document\n","            for word, freq in word_freq.items():  # for each unique word in the review.                \n","                if len(word) < 2:\n","                    continue\n","                # we will check if its there in the vocabulary that we build in fit() function\n","                # dict.get() function will return the values, if the key doesn't exits it will return -1\n","                col_index = vocab.get(word, -1) # retreving the dimension number of a word\n","                # if the word exists\n","                if col_index !=-1:\n","                    # we are storing the index of the document\n","                    rows.append(idx)\n","                    # we are storing the dimensions of the word\n","                    columns.append(col_index)\n","                    # we are storing the frequency of the word\n","                    values.append(freq)\n","        return csr_matrix((values, (rows,columns)), shape=(len(dataset),len(vocab)))\n","    else:\n","        print(\"you need to pass list of strings\")\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"HjuCcJwXxLjJ","colab":{},"outputId":"c1dab984-1250-4e7a-a788-e81565450bd1"},"source":["# Write your code here.\n","# Make sure its well documented and readble with appropriate comments.\n","# Compare your results with the above sklearn tfidf vectorizer\n","# You are not supposed to use any other library apart from the ones given below\n","\n","from collections import Counter\n","from tqdm import tqdm\n","from scipy.sparse import csr_matrix\n","import math\n","import operator\n","from sklearn.preprocessing import normalize\n","import numpy as np\n","corpus = [\n","     'this is the first document',\n","     'this document is the second document',\n","     'and this is the third one',\n","     'is this the first document',\n","]\n","lista=[]\n","def fit(dataset):    \n","    unique_words = set() # at first we will initialize an empty set\n","    # check if its list type or not\n","    if isinstance(dataset, (list,)):\n","        for row in dataset: # for each review in the dataset\n","            for word in row.split(\" \"): # for each word in the review. #split method converts a string into list of words\n","                if len(word) < 2:\n","                    continue\n","                unique_words.add(word)\n","        unique_words = sorted(list(unique_words))\n","        vocab = {j:i for i,j in enumerate(unique_words)}\n","        \n","        dictii=[]\n","          \n","        for i in corpus:\n","            for j in i.split(\" \"):\n","                lista.append(j)\n","        lista.sort()\n","        dicti=Counter(lista)\n","        #print(dicti)\n","        count=0\n","        for k in dicti.values():\n","            count=count+k\n","        countr=0\n","        countf=[]\n","        listd=[]\n","        listc=[]\n","        tf=[]\n","        for i in corpus:\n","            for j in i.split(\" \"):\n","                countr=countr+1\n","            countf.append(countr)\n","            countr=0\n","            \n","        for i in corpus:\n","            for j in i.split(\" \"):\n","                listd.append(j)\n","                \n","            dictii.append(Counter(listd))\n","            listd.clear()\n","       \n","        for i in dictii:\n","            ra={}\n","            for k in i: \n","                f=i[k]\n","                ra.update({k:f})\n","            tf.append(ra)\n","        #print(tf)\n","        #print(dictii)\n","     \n","        tf_rev=[]\n","        m=0\n","        for i in tf:\n","            \n","            rb={}\n","            for j in i:\n","                s=i[j]\n","                rb.update({j:(s/countf[m])})\n","                   \n","            tf_rev.append(rb)\n","            m=m+1\n","        #print(tf_rev)\n","                \n","        \n","            \n","            \n","       \n","        s=0\n","        listb=[]\n","        gb=[]\n","        n=len(corpus)\n","        for k,v in dicti.items():\n","            for i in corpus:\n","                g={}\n","                for j in i.split(\" \"):\n","                    if k in j:\n","                        s=s+1\n","                        break\n","                        \n","                    else:continue\n","                g.update({k:s})\n","            gb.append(g)\n","            listb.append(s)\n","            s=0\n","        s=list(dicti.keys())\n","        lists=(transform(corpus, vocab).toarray())\n","        print(s)\n","        print(\"\\n\")\n","        \n","        idf_dict={}\n","        for k in gb:\n","            for p,q in k.items():\n","                f=1+math.log((1+n)/(1+k[p]))\n","                idf_dict.update({p:f})\n","        #print(idf_dict)\n","        \n","        idf=[]\n","        for i in listb:\n","            f=1+math.log((1+n)/(1+i))\n","            idf.append(f)\n","        print(idf)\n","        print(\"\\n\")\n","        print(lists.shape)\n","        print(\"\\n\")\n","    \n","    #print(gb)\n","    \n","        \n","    fetch=[]\n","    for i in range(len(lists)):\n","        rq=[]\n","        for j in range(len(lists[i])):\n","            if lists[i][j]>0:\n","                     rq.append(s[j])\n","            else:\n","                 rq.append(0)\n","        fetch.append(rq)\n","    #print(fetch) \n","    mn=[]\n","    countr=0\n","    for i in range(len(fetch)):\n","        \n","        for k in range(len(tf_rev)):\n","                mg=tf_rev[k]\n","                #print(mg)\n","                rc=[]\n","                for h,g in mg.items():\n","                   \n","                    #print(mg[h])\n","                    \n","                    for j in range(len(fetch[i])):\n","                        #print(fetch[i][j])\n","                        if fetch[i][j]==h:\n","                            rc.append(g)\n","                        \n","                        \n","                        \n","                mn.append(rc)\n","                \n","    #print(mn)\n","    pq=[]\n","    for i in range(len(fetch)):\n","        rd=[]\n","        for h,g in idf_dict.items():\n","            for j in range(len(fetch[i])):\n","                       # print(fetch[i][j])\n","                        if fetch[i][j]==h:\n","                            rd.append(g)\n","                        \n","                        \n","        pq.append(rd)\n","    #print(pq)\n","    res=[]\n","    for i in range(4):\n","        hg=[]\n","        for j in range(len(mn[i])):\n","            \n","            hg.append(mn[i][j]*pq[i][j])\n","        \n","        res.append(hg)\n","    #print(res)\n","    rb=[]\n","    for i in range(len(fetch)):\n","        for j in range(len(fetch[i])):\n","            if fetch[i][j]==0:\n","                #print(fetch.index(fetch[i][j]))\n","                res[i].insert(j,0)\n","    #print(res)\n","              \n","    \n","    ui=[]\n","    for i in res:\n","        \n","        rb=[i]\n","        ui.append(normalize(rb))\n","    \n","   # print(ui)\n","    for i in range(len(ui[0])):\n","        m=9\n","        for k in ui[0][i][::-1]:\n","            m=m-1\n","            if k>0:\n","                #m=list(ui[0][i]).index(k)\n","                print(\"({},{})  {}\".format(0,m,k))\n","                print(\"\\n\")\n","    print(ui[0])\n","  \n","    return vocab \n","\n","vocab = fit(corpus)\n","#print(vocab)\n","#print(list(vocab.keys()))\n","lists=[]\n","lists=(transform(corpus, vocab).toarray())\n","\n","\n","#print(lists)\n","\n","lists.shape"],"execution_count":0,"outputs":[{"output_type":"stream","text":["100%|██████████| 4/4 [00:00<00:00, 3991.72it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n","\n","\n","[1.916290731874155, 1.2231435513142097, 1.5108256237659907, 1.0, 1.916290731874155, 1.916290731874155, 1.0, 1.916290731874155, 1.0]\n","\n","\n","(4, 9)\n","\n","\n","(0,8)  0.3840852409148149\n","\n","\n","(0,6)  0.3840852409148149\n","\n","\n","(0,3)  0.3840852409148149\n","\n","\n","(0,2)  0.580285823684436\n","\n","\n","(0,1)  0.4697913855799205\n","\n","\n","[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n","  0.38408524 0.         0.38408524]]\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 4/4 [00:00<00:00, 3996.48it/s]\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["(4, 9)"]},"metadata":{"tags":[]},"execution_count":120}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"MMxBmVZExLjK"},"source":["## Task-2"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"51j_OtqAxLjL"},"source":["<font face='georgia'>\n","    <h4><strong>2. Implement max features functionality:</strong></h4>\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"NHxPLlwNxLjL","outputId":"9abd8e08-0e24-4975-9a13-4d3636d60323","colab":{}},"source":["# Below is the code to load the cleaned_strings pickle file provided\n","# Here corpus is of list type\n","\n","import pickle\n","with open('cleaned_strings', 'rb') as f:\n","    corpus = pickle.load(f)\n","    \n","# printing the length of the corpus loaded\n","print(\"Number of documents in corpus = \",len(corpus))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Number of documents in corpus =  746\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ZULfoOIdxLjQ","colab":{}},"source":["# Write your code here.\n","# Try not to hardcode any values.\n","# Make sure its well documented and readble with appropriate comments."],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"1_DJnnR3xLjR","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7XHj7WZxx3f7","colab_type":"code","colab":{}},"source":["def transform2(dataset,vocab):\n","    rows = []\n","    columns = []\n","    values = []\n","    if isinstance(dataset, (list,)):\n","        for idx, row in enumerate(tqdm(dataset)): # for each document in the dataset\n","            # it will return a dict type object where key is the word and values is its frequency, {word:frequency}\n","            word_freq = dict(Counter(row.split()))\n","            # for every unique word in the document\n","            for word, freq in word_freq.items():  # for each unique word in the review.                \n","                if len(word) < 2:\n","                    continue\n","                # we will check if its there in the vocabulary that we build in fit() function\n","                # dict.get() function will return the values, if the key doesn't exits it will return -1\n","                col_index = vocab.get(word, -1) # retreving the dimension number of a word\n","                # if the word exists\n","                if col_index !=-1:\n","                    # we are storing the index of the document\n","                    rows.append(idx)\n","                    # we are storing the dimensions of the word\n","                    columns.append(col_index)\n","                    # we are storing the frequency of the word\n","                    values.append(freq)\n","        return csr_matrix((values, (rows,columns)), shape=(len(dataset),len(vocab)))\n","    else:\n","        print(\"you need to pass list of strings\")\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Dc7O3PVjx3f-","colab_type":"code","colab":{},"outputId":"b11568be-dbdc-4bb7-e7a8-2bd2372c8a3b"},"source":["# Write your code here.\n","# Make sure its well documented and readble with appropriate comments.\n","# Compare your results with the above sklearn tfidf vectorizer\n","# You are not supposed to use any other library apart from the ones given below\n","\n","from collections import Counter\n","from tqdm import tqdm\n","from scipy.sparse import csr_matrix\n","import math\n","import operator\n","from sklearn.preprocessing import normalize\n","import numpy as np\n","import pickle\n","with open('cleaned_strings', 'rb') as f:\n","    corpus2 = pickle.load(f)\n","    \n","# printing the length of the corpus loaded\n","print(\"Number of documents in corpus = \",len(corpus2))\n","lista=[]\n","def fit2(dataset):    \n","    unique_words = set() # at first we will initialize an empty set\n","    # check if its list type or not\n","    if isinstance(dataset, (list,)):\n","        for row in dataset: # for each review in the dataset\n","            for word in row.split(\" \"): # for each word in the review. #split method converts a string into list of words\n","                if len(word) < 2:\n","                    continue\n","                unique_words.add(word)\n","        unique_words = sorted(list(unique_words))\n","        vocab2 = {j:i for i,j in enumerate(unique_words)}\n","        \n","        dictii=[]\n","          \n","        for i in corpus2:\n","            for j in i.split(\" \"):\n","                lista.append(j)\n","        lista.sort()\n","        dicti=Counter(lista)\n","        #print(dicti)\n","        count=0\n","        for k in dicti.values():\n","            count=count+k\n","        countr=0\n","        countf=[]\n","        listd=[]\n","        listc=[]\n","        tf=[]\n","        for i in corpus2:\n","            for j in i.split(\" \"):\n","                countr=countr+1\n","            countf.append(countr)\n","            countr=0\n","            \n","        for i in corpus2:\n","            for j in i.split(\" \"):\n","                listd.append(j)\n","                \n","            dictii.append(Counter(listd))\n","            listd.clear()\n","       \n","        for i in dictii:\n","            ra={}\n","            for k in i: \n","                f=i[k]\n","                ra.update({k:f})\n","            tf.append(ra)\n","        #print(tf)\n","        #print(dictii)\n","        tf_rev=[]\n","        m=0\n","        for i in tf:\n","            \n","            rb={}\n","            for j in i:\n","                s=i[j]\n","                rb.update({j:(s/countf[m])})\n","                   \n","            tf_rev.append(rb)\n","            m=m+1\n","        #print(tf_rev)\n","        gb=[]\n","        n=len(corpus2)\n","        listb=[]\n","        for k,v in dicti.items():\n","            for i in corpus2:\n","                g={}\n","                for j in i.split(\" \"):\n","                    if k in j:\n","                        s=s+1\n","                        break\n","                        \n","                    else:continue\n","                g.update({k:s})\n","            gb.append(g)\n","            listb.append(s)\n","            s=0\n","        \n","        \n","        #print(s)\n","        #print(\"\\n\")\n","        s=0\n","        listb=[]\n","        \n","        \n","        idf_dict={}\n","        for k in gb:\n","            for p,q in k.items():\n","                f=1+math.log((1+n)/(1+k[p]))\n","                idf_dict.update({p:f})\n","        #print(idf_dict)\n","        listi=[sorted(idf_dict.items(),key=lambda x:x[1],reverse=True)[:50]]\n","        print(listi)#both string and number\n","        #top 50 vocab\n","        vocab_50=[]#only strings\n","        for i in range(len(listi)):\n","            for j in range(len(listi[i])):\n","                vocab_50.append(listi[i][j][0])\n","        #print(vocab_50)\n","        unique_words_50 = set()\n","        unique_words_50 = sorted(list(vocab_50))\n","        #print(unique_words_50)\n","        j=i=1\n","        vocab2_50 = {j:i for i,j in enumerate(unique_words_50,start=0)}\n","        #print(vocab2_50)\n","        lists_50=(transform2(corpus2, vocab2_50).toarray())\n","        print(lists_50)\n","        print(lists_50.shape)\n","        rb=[]\n","        ui=[]\n","        for i in lists_50:\n","            rb=[i]\n","            ui.append(normalize(rb))\n","        #print(ui)\n","        \n","        print(ui[0][0])\n","        \n","        \n","        \n","        \n","        \n","    return vocab2"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Number of documents in corpus =  746\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0IqZx-oox3gC","colab_type":"code","colab":{},"outputId":"e8d4b7cb-88a9-4b6d-8607-d3fe2c3fada2"},"source":["vocab2 = fit2(corpus2)\n","#print(vocab2)\n","#print(transform(corpus2, vocab2_50).toarray())\n","\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[('abandoned', 6.922918004572872), ('abroad', 6.922918004572872), ('abstruse', 6.922918004572872), ('academy', 6.922918004572872), ('accents', 6.922918004572872), ('accessible', 6.922918004572872), ('acclaimed', 6.922918004572872), ('accolades', 6.922918004572872), ('accurately', 6.922918004572872), ('achille', 6.922918004572872), ('ackerman', 6.922918004572872), ('adams', 6.922918004572872), ('added', 6.922918004572872), ('admins', 6.922918004572872), ('admiration', 6.922918004572872), ('admitted', 6.922918004572872), ('adrift', 6.922918004572872), ('adventure', 6.922918004572872), ('aesthetically', 6.922918004572872), ('affected', 6.922918004572872), ('affleck', 6.922918004572872), ('afternoon', 6.922918004572872), ('agreed', 6.922918004572872), ('aimless', 6.922918004572872), ('aired', 6.922918004572872), ('akasha', 6.922918004572872), ('alert', 6.922918004572872), ('alike', 6.922918004572872), ('allison', 6.922918004572872), ('allowing', 6.922918004572872), ('alongside', 6.922918004572872), ('amateurish', 6.922918004572872), ('amazed', 6.922918004572872), ('amazingly', 6.922918004572872), ('amusing', 6.922918004572872), ('amust', 6.922918004572872), ('anatomist', 6.922918004572872), ('angela', 6.922918004572872), ('angelina', 6.922918004572872), ('angry', 6.922918004572872), ('anguish', 6.922918004572872), ('angus', 6.922918004572872), ('animals', 6.922918004572872), ('animated', 6.922918004572872), ('anita', 6.922918004572872), ('anniversary', 6.922918004572872), ('anthony', 6.922918004572872), ('antithesis', 6.922918004572872), ('anyway', 6.922918004572872), ('apart', 6.922918004572872)]]\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 746/746 [00:00<00:00, 24046.84it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["[[0 0 0 ... 0 0 0]\n"," [0 0 0 ... 0 0 0]\n"," [0 0 0 ... 0 0 0]\n"," ...\n"," [0 0 0 ... 0 0 0]\n"," [0 0 0 ... 0 0 0]\n"," [0 0 0 ... 0 0 0]]\n","(746, 50)\n","[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0.]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3YWzRoYYx3gK","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6jlXW9Oex3gN","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}