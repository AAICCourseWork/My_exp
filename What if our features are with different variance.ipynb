{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"What if our features are with different variance.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"iCQexd1a2F3D","colab_type":"code","colab":{}},"source":["import numpy as np\n","import pandas as pd\n","#import plotly\n","#import plotly.figure_factory as ff\n","#import plotly.graph_objs as go\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.preprocessing import MinMaxScaler\n","#from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n","#init_notebook_mode(connected=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5I2XC94p2F3K","colab_type":"code","colab":{}},"source":["data = pd.read_csv('task_b.csv')\n","data=data.iloc[:,1:]\n","y = data['y'].values\n","X = data.drop(['y'], axis=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"eStU0uHq2F3O","colab_type":"code","colab":{},"outputId":"1365c837-3ad2-46cf-de16-76e700d451c6"},"source":["data.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>f1</th>\n","      <th>f2</th>\n","      <th>f3</th>\n","      <th>y</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>-195.871045</td>\n","      <td>-14843.084171</td>\n","      <td>5.532140</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>-1217.183964</td>\n","      <td>-4068.124621</td>\n","      <td>4.416082</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>9.138451</td>\n","      <td>4413.412028</td>\n","      <td>0.425317</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>363.824242</td>\n","      <td>15474.760647</td>\n","      <td>1.094119</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>-768.812047</td>\n","      <td>-7963.932192</td>\n","      <td>1.870536</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            f1            f2        f3    y\n","0  -195.871045 -14843.084171  5.532140  1.0\n","1 -1217.183964  -4068.124621  4.416082  1.0\n","2     9.138451   4413.412028  0.425317  0.0\n","3   363.824242  15474.760647  1.094119  0.0\n","4  -768.812047  -7963.932192  1.870536  0.0"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"pzf7jIJA2F3T","colab_type":"code","colab":{},"outputId":"6c71e179-8c2a-4183-a271-ce16ca74ef33"},"source":["print(data.corr()['y'])\n","#we can observe that f3 is more correlated to y than the other features\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["f1    0.067172\n","f2   -0.017944\n","f3    0.839060\n","y     1.000000\n","Name: y, dtype: float64\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"scrolled":true,"id":"xFFPNJ122F3X","colab_type":"code","colab":{},"outputId":"f1be93a0-042f-40ff-9579-911d7ee5e37f"},"source":["data.std()\n","#var(F2)>>var(F1)>data.std()>Var(F3)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["f1      488.195035\n","f2    10403.417325\n","f3        2.926662\n","y         0.501255\n","dtype: float64"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"wJ40HFX42F3c","colab_type":"code","colab":{},"outputId":"a7e35462-c4a4-47ad-ae63-9d1f77e4b3e9"},"source":["X=data[['f1','f2','f3']].values\n","Y=data['y'].values\n","print(X.shape)\n","print(Y.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(200, 3)\n","(200,)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ufAIAgqa2F3g","colab_type":"text"},"source":["# What if our features are with different variance \n","\n","<pre>\n","\n","\n","> <b>Task1</b>:\n","    1. Apply Logistic regression(SGDClassifier with logloss) on 'data' and check the feature importance\n","    2. Apply SVM(SGDClassifier with hinge) on 'data' and check the feature importance\n","\n","> <b>Task2</b>:\n","    1. Apply Logistic regression(SGDClassifier with logloss) on 'data' after standardization \n","       i.e standardization(data, column wise): (column-mean(column))/std(column) and check the feature importance\n","    2. Apply SVM(SGDClassifier with hinge) on 'data' after standardization \n","       i.e standardization(data, column wise): (column-mean(column))/std(column) and check the feature importance\n","\n","</pre>"]},{"cell_type":"code","metadata":{"id":"-SXU6WYd2F3h","colab_type":"code","colab":{},"outputId":"001474c2-1292-49a9-e7cb-fde765190c85"},"source":["from sklearn.model_selection import train_test_split\n","from sklearn import linear_model\n","X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.25, random_state=15)\n","clf = linear_model.SGDClassifier(eta0=0.0001, alpha=0.0001, loss='log', random_state=15, penalty='l2', tol=1e-3, verbose=2, learning_rate='constant')\n","clf.fit(X=X_train, y=y_train)\n","clf.coef_\n","#since feature f1 and f2 as high variance it as more negative weight compared to other features\n","#i.e.,var(F2)>>var(F1)>>Var(F3) implie Weight f2>weight f1>weight f3 before stanradization"],"execution_count":0,"outputs":[{"output_type":"stream","text":["-- Epoch 1\n","Norm: 0.64, NNZs: 3, Bias: -0.000451, T: 150, Avg. loss: 2519.859907\n","Total training time: 0.00 seconds.\n","-- Epoch 2\n","Norm: 0.44, NNZs: 3, Bias: 0.000454, T: 300, Avg. loss: 3700.524704\n","Total training time: 0.00 seconds.\n","-- Epoch 3\n","Norm: 1.61, NNZs: 3, Bias: -0.000646, T: 450, Avg. loss: 2668.580474\n","Total training time: 0.00 seconds.\n","-- Epoch 4\n","Norm: 1.06, NNZs: 3, Bias: 0.000054, T: 600, Avg. loss: 2724.175085\n","Total training time: 0.00 seconds.\n","-- Epoch 5\n","Norm: 1.46, NNZs: 3, Bias: -0.000546, T: 750, Avg. loss: 1659.479597\n","Total training time: 0.00 seconds.\n","-- Epoch 6\n","Norm: 0.91, NNZs: 3, Bias: -0.001246, T: 900, Avg. loss: 3283.508318\n","Total training time: 0.00 seconds.\n","-- Epoch 7\n","Norm: 1.14, NNZs: 3, Bias: -0.000546, T: 1050, Avg. loss: 2599.108764\n","Total training time: 0.00 seconds.\n","-- Epoch 8\n","Norm: 0.17, NNZs: 3, Bias: -0.000946, T: 1200, Avg. loss: 3587.171630\n","Total training time: 0.00 seconds.\n","-- Epoch 9\n","Norm: 0.67, NNZs: 3, Bias: -0.001746, T: 1350, Avg. loss: 3222.126803\n","Total training time: 0.00 seconds.\n","-- Epoch 10\n","Norm: 0.72, NNZs: 3, Bias: -0.001210, T: 1500, Avg. loss: 2890.385311\n","Total training time: 0.00 seconds.\n","Convergence after 10 epochs took 0.00 seconds\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["array([[-0.03775828, -0.70442478,  0.16523703]])"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"MFM73ElE2F3l","colab_type":"code","colab":{},"outputId":"464b0b19-436d-4d80-e745-319e76904217"},"source":["from sklearn.model_selection import train_test_split\n","from sklearn import linear_model\n","X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.25, random_state=15)\n","clf = linear_model.SGDClassifier(eta0=0.0001, alpha=0.0001, loss='hinge', random_state=15, penalty='l2', tol=1e-3, verbose=2, learning_rate='constant')\n","clf.fit(X=X_train, y=y_train)\n","clf.coef_\n","#since feature f1 and f2 as high variance it as more  weight compared to other features\n","#i.e.,var(F2)>>var(F1)>>Var(F3) implie Weight f2>weight f1>weight f3 before stanradization"],"execution_count":0,"outputs":[{"output_type":"stream","text":["-- Epoch 1\n","Norm: 0.74, NNZs: 3, Bias: 0.000200, T: 150, Avg. loss: 2553.490327\n","Total training time: 0.00 seconds.\n","-- Epoch 2\n","Norm: 0.73, NNZs: 3, Bias: 0.001100, T: 300, Avg. loss: 3545.641692\n","Total training time: 0.02 seconds.\n","-- Epoch 3\n","Norm: 1.57, NNZs: 3, Bias: 0.000000, T: 450, Avg. loss: 2801.806688\n","Total training time: 0.02 seconds.\n","-- Epoch 4\n","Norm: 0.82, NNZs: 3, Bias: -0.000300, T: 600, Avg. loss: 3235.468023\n","Total training time: 0.02 seconds.\n","-- Epoch 5\n","Norm: 1.48, NNZs: 3, Bias: -0.000800, T: 750, Avg. loss: 1732.864446\n","Total training time: 0.02 seconds.\n","-- Epoch 6\n","Norm: 0.97, NNZs: 3, Bias: -0.001500, T: 900, Avg. loss: 3282.645554\n","Total training time: 0.02 seconds.\n","-- Epoch 7\n","Norm: 1.41, NNZs: 3, Bias: -0.000700, T: 1050, Avg. loss: 2333.759393\n","Total training time: 0.02 seconds.\n","-- Epoch 8\n","Norm: 0.71, NNZs: 3, Bias: -0.001300, T: 1200, Avg. loss: 3395.312052\n","Total training time: 0.02 seconds.\n","-- Epoch 9\n","Norm: 0.89, NNZs: 3, Bias: -0.001500, T: 1350, Avg. loss: 3095.883588\n","Total training time: 0.03 seconds.\n","-- Epoch 10\n","Norm: 0.77, NNZs: 3, Bias: -0.001400, T: 1500, Avg. loss: 2432.072916\n","Total training time: 0.03 seconds.\n","Convergence after 10 epochs took 0.03 seconds\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["array([[ 0.19738792, -0.72445957,  0.17001895]])"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"code","metadata":{"id":"Mn_ntcYZ2F3o","colab_type":"code","colab":{},"outputId":"8825da60-15e4-4244-8531-4108276108ac"},"source":["from sklearn import preprocessing\n","from sklearn.model_selection import train_test_split\n","from sklearn import linear_model\n","X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.25, random_state=15)\n","clf = linear_model.SGDClassifier(eta0=0.0001, alpha=0.0001, loss='log', random_state=15, penalty='l2', tol=1e-3, verbose=2, learning_rate='constant')\n","scaler = StandardScaler()\n","X_std = scaler.fit_transform(X_train)\n","#Y_std = scaler.fit_transform( y_train)\n","clf.fit(X=X_std, y=y_train)\n","clf.coef_\n","\n","#since feature f1 and f2 as high variance compared to f3 after standarization its weight reduced compared to  feature f3\n","#i.e.,var(F2)>>var(F1)>>Var(F3) implie Weight f2<weight f1<weight f3 after stanradization"],"execution_count":0,"outputs":[{"output_type":"stream","text":["-- Epoch 1\n","Norm: 0.01, NNZs: 3, Bias: -0.000000, T: 150, Avg. loss: 0.691900\n","Total training time: 0.00 seconds.\n","-- Epoch 2\n","Norm: 0.01, NNZs: 3, Bias: -0.000000, T: 300, Avg. loss: 0.689318\n","Total training time: 0.00 seconds.\n","-- Epoch 3\n","Norm: 0.02, NNZs: 3, Bias: -0.000001, T: 450, Avg. loss: 0.686755\n","Total training time: 0.00 seconds.\n","-- Epoch 4\n","Norm: 0.02, NNZs: 3, Bias: -0.000001, T: 600, Avg. loss: 0.684213\n","Total training time: 0.00 seconds.\n","-- Epoch 5\n","Norm: 0.03, NNZs: 3, Bias: -0.000001, T: 750, Avg. loss: 0.681690\n","Total training time: 0.00 seconds.\n","-- Epoch 6\n","Norm: 0.04, NNZs: 3, Bias: -0.000001, T: 900, Avg. loss: 0.679187\n","Total training time: 0.00 seconds.\n","-- Epoch 7\n","Norm: 0.04, NNZs: 3, Bias: -0.000002, T: 1050, Avg. loss: 0.676702\n","Total training time: 0.00 seconds.\n","-- Epoch 8\n","Norm: 0.05, NNZs: 3, Bias: -0.000001, T: 1200, Avg. loss: 0.674237\n","Total training time: 0.00 seconds.\n","-- Epoch 9\n","Norm: 0.06, NNZs: 3, Bias: -0.000001, T: 1350, Avg. loss: 0.671791\n","Total training time: 0.00 seconds.\n","-- Epoch 10\n","Norm: 0.06, NNZs: 3, Bias: -0.000001, T: 1500, Avg. loss: 0.669364\n","Total training time: 0.00 seconds.\n","-- Epoch 11\n","Norm: 0.07, NNZs: 3, Bias: -0.000001, T: 1650, Avg. loss: 0.666955\n","Total training time: 0.00 seconds.\n","-- Epoch 12\n","Norm: 0.07, NNZs: 3, Bias: -0.000001, T: 1800, Avg. loss: 0.664565\n","Total training time: 0.00 seconds.\n","-- Epoch 13\n","Norm: 0.08, NNZs: 3, Bias: -0.000001, T: 1950, Avg. loss: 0.662193\n","Total training time: 0.00 seconds.\n","-- Epoch 14\n","Norm: 0.09, NNZs: 3, Bias: -0.000000, T: 2100, Avg. loss: 0.659839\n","Total training time: 0.00 seconds.\n","-- Epoch 15\n","Norm: 0.09, NNZs: 3, Bias: -0.000000, T: 2250, Avg. loss: 0.657503\n","Total training time: 0.00 seconds.\n","-- Epoch 16\n","Norm: 0.10, NNZs: 3, Bias: 0.000000, T: 2400, Avg. loss: 0.655185\n","Total training time: 0.01 seconds.\n","-- Epoch 17\n","Norm: 0.10, NNZs: 3, Bias: -0.000000, T: 2550, Avg. loss: 0.652884\n","Total training time: 0.01 seconds.\n","-- Epoch 18\n","Norm: 0.11, NNZs: 3, Bias: -0.000001, T: 2700, Avg. loss: 0.650602\n","Total training time: 0.01 seconds.\n","-- Epoch 19\n","Norm: 0.11, NNZs: 3, Bias: -0.000001, T: 2850, Avg. loss: 0.648337\n","Total training time: 0.01 seconds.\n","-- Epoch 20\n","Norm: 0.12, NNZs: 3, Bias: -0.000001, T: 3000, Avg. loss: 0.646089\n","Total training time: 0.01 seconds.\n","-- Epoch 21\n","Norm: 0.13, NNZs: 3, Bias: -0.000001, T: 3150, Avg. loss: 0.643858\n","Total training time: 0.01 seconds.\n","-- Epoch 22\n","Norm: 0.13, NNZs: 3, Bias: -0.000001, T: 3300, Avg. loss: 0.641644\n","Total training time: 0.01 seconds.\n","-- Epoch 23\n","Norm: 0.14, NNZs: 3, Bias: -0.000000, T: 3450, Avg. loss: 0.639446\n","Total training time: 0.01 seconds.\n","-- Epoch 24\n","Norm: 0.14, NNZs: 3, Bias: -0.000000, T: 3600, Avg. loss: 0.637266\n","Total training time: 0.01 seconds.\n","-- Epoch 25\n","Norm: 0.15, NNZs: 3, Bias: 0.000000, T: 3750, Avg. loss: 0.635102\n","Total training time: 0.01 seconds.\n","-- Epoch 26\n","Norm: 0.15, NNZs: 3, Bias: 0.000000, T: 3900, Avg. loss: 0.632955\n","Total training time: 0.01 seconds.\n","-- Epoch 27\n","Norm: 0.16, NNZs: 3, Bias: -0.000001, T: 4050, Avg. loss: 0.630823\n","Total training time: 0.01 seconds.\n","-- Epoch 28\n","Norm: 0.17, NNZs: 3, Bias: -0.000001, T: 4200, Avg. loss: 0.628708\n","Total training time: 0.01 seconds.\n","-- Epoch 29\n","Norm: 0.17, NNZs: 3, Bias: -0.000001, T: 4350, Avg. loss: 0.626609\n","Total training time: 0.01 seconds.\n","-- Epoch 30\n","Norm: 0.18, NNZs: 3, Bias: -0.000002, T: 4500, Avg. loss: 0.624526\n","Total training time: 0.01 seconds.\n","-- Epoch 31\n","Norm: 0.18, NNZs: 3, Bias: -0.000001, T: 4650, Avg. loss: 0.622459\n","Total training time: 0.01 seconds.\n","-- Epoch 32\n","Norm: 0.19, NNZs: 3, Bias: -0.000002, T: 4800, Avg. loss: 0.620407\n","Total training time: 0.01 seconds.\n","-- Epoch 33\n","Norm: 0.19, NNZs: 3, Bias: -0.000002, T: 4950, Avg. loss: 0.618370\n","Total training time: 0.01 seconds.\n","-- Epoch 34\n","Norm: 0.20, NNZs: 3, Bias: -0.000003, T: 5100, Avg. loss: 0.616349\n","Total training time: 0.01 seconds.\n","-- Epoch 35\n","Norm: 0.20, NNZs: 3, Bias: -0.000003, T: 5250, Avg. loss: 0.614343\n","Total training time: 0.01 seconds.\n","-- Epoch 36\n","Norm: 0.21, NNZs: 3, Bias: -0.000003, T: 5400, Avg. loss: 0.612352\n","Total training time: 0.01 seconds.\n","-- Epoch 37\n","Norm: 0.22, NNZs: 3, Bias: -0.000003, T: 5550, Avg. loss: 0.610376\n","Total training time: 0.01 seconds.\n","-- Epoch 38\n","Norm: 0.22, NNZs: 3, Bias: -0.000002, T: 5700, Avg. loss: 0.608415\n","Total training time: 0.01 seconds.\n","-- Epoch 39\n","Norm: 0.23, NNZs: 3, Bias: -0.000002, T: 5850, Avg. loss: 0.606468\n","Total training time: 0.01 seconds.\n","-- Epoch 40\n","Norm: 0.23, NNZs: 3, Bias: -0.000001, T: 6000, Avg. loss: 0.604536\n","Total training time: 0.01 seconds.\n","-- Epoch 41\n","Norm: 0.24, NNZs: 3, Bias: -0.000002, T: 6150, Avg. loss: 0.602619\n","Total training time: 0.01 seconds.\n","-- Epoch 42\n","Norm: 0.24, NNZs: 3, Bias: -0.000002, T: 6300, Avg. loss: 0.600715\n","Total training time: 0.01 seconds.\n","-- Epoch 43\n","Norm: 0.25, NNZs: 3, Bias: -0.000002, T: 6450, Avg. loss: 0.598826\n","Total training time: 0.01 seconds.\n","-- Epoch 44\n","Norm: 0.25, NNZs: 3, Bias: -0.000001, T: 6600, Avg. loss: 0.596951\n","Total training time: 0.01 seconds.\n","-- Epoch 45\n","Norm: 0.26, NNZs: 3, Bias: -0.000002, T: 6750, Avg. loss: 0.595089\n","Total training time: 0.02 seconds.\n","-- Epoch 46\n","Norm: 0.26, NNZs: 3, Bias: -0.000002, T: 6900, Avg. loss: 0.593242\n","Total training time: 0.02 seconds.\n","-- Epoch 47\n","Norm: 0.27, NNZs: 3, Bias: -0.000001, T: 7050, Avg. loss: 0.591408\n","Total training time: 0.02 seconds.\n","-- Epoch 48\n","Norm: 0.27, NNZs: 3, Bias: -0.000002, T: 7200, Avg. loss: 0.589588\n","Total training time: 0.02 seconds.\n","-- Epoch 49\n","Norm: 0.28, NNZs: 3, Bias: -0.000002, T: 7350, Avg. loss: 0.587781\n","Total training time: 0.02 seconds.\n","-- Epoch 50\n","Norm: 0.28, NNZs: 3, Bias: -0.000002, T: 7500, Avg. loss: 0.585987\n","Total training time: 0.02 seconds.\n","-- Epoch 51\n","Norm: 0.29, NNZs: 3, Bias: -0.000002, T: 7650, Avg. loss: 0.584207\n","Total training time: 0.02 seconds.\n","-- Epoch 52\n","Norm: 0.29, NNZs: 3, Bias: -0.000003, T: 7800, Avg. loss: 0.582440\n","Total training time: 0.02 seconds.\n","-- Epoch 53\n","Norm: 0.30, NNZs: 3, Bias: -0.000003, T: 7950, Avg. loss: 0.580685\n","Total training time: 0.02 seconds.\n","-- Epoch 54\n","Norm: 0.30, NNZs: 3, Bias: -0.000003, T: 8100, Avg. loss: 0.578944\n","Total training time: 0.02 seconds.\n","-- Epoch 55\n","Norm: 0.31, NNZs: 3, Bias: -0.000003, T: 8250, Avg. loss: 0.577215\n","Total training time: 0.02 seconds.\n","-- Epoch 56\n","Norm: 0.31, NNZs: 3, Bias: -0.000004, T: 8400, Avg. loss: 0.575499\n","Total training time: 0.02 seconds.\n","-- Epoch 57\n","Norm: 0.32, NNZs: 3, Bias: -0.000005, T: 8550, Avg. loss: 0.573795\n","Total training time: 0.02 seconds.\n","-- Epoch 58\n","Norm: 0.33, NNZs: 3, Bias: -0.000005, T: 8700, Avg. loss: 0.572104\n","Total training time: 0.02 seconds.\n","-- Epoch 59\n","Norm: 0.33, NNZs: 3, Bias: -0.000005, T: 8850, Avg. loss: 0.570425\n","Total training time: 0.02 seconds.\n","-- Epoch 60\n","Norm: 0.34, NNZs: 3, Bias: -0.000006, T: 9000, Avg. loss: 0.568758\n","Total training time: 0.02 seconds.\n","-- Epoch 61\n","Norm: 0.34, NNZs: 3, Bias: -0.000007, T: 9150, Avg. loss: 0.567103\n","Total training time: 0.02 seconds.\n","-- Epoch 62\n","Norm: 0.34, NNZs: 3, Bias: -0.000008, T: 9300, Avg. loss: 0.565460\n","Total training time: 0.02 seconds.\n","-- Epoch 63\n","Norm: 0.35, NNZs: 3, Bias: -0.000008, T: 9450, Avg. loss: 0.563829\n","Total training time: 0.02 seconds.\n","-- Epoch 64\n","Norm: 0.35, NNZs: 3, Bias: -0.000008, T: 9600, Avg. loss: 0.562210\n","Total training time: 0.02 seconds.\n","-- Epoch 65\n","Norm: 0.36, NNZs: 3, Bias: -0.000008, T: 9750, Avg. loss: 0.560602\n","Total training time: 0.02 seconds.\n","-- Epoch 66\n","Norm: 0.36, NNZs: 3, Bias: -0.000008, T: 9900, Avg. loss: 0.559006\n","Total training time: 0.02 seconds.\n","-- Epoch 67\n","Norm: 0.37, NNZs: 3, Bias: -0.000008, T: 10050, Avg. loss: 0.557421\n","Total training time: 0.02 seconds.\n","-- Epoch 68\n","Norm: 0.37, NNZs: 3, Bias: -0.000009, T: 10200, Avg. loss: 0.555848\n","Total training time: 0.02 seconds.\n","-- Epoch 69\n","Norm: 0.38, NNZs: 3, Bias: -0.000009, T: 10350, Avg. loss: 0.554286\n","Total training time: 0.02 seconds.\n","-- Epoch 70\n","Norm: 0.38, NNZs: 3, Bias: -0.000010, T: 10500, Avg. loss: 0.552734\n","Total training time: 0.02 seconds.\n","-- Epoch 71\n","Norm: 0.39, NNZs: 3, Bias: -0.000010, T: 10650, Avg. loss: 0.551194\n","Total training time: 0.02 seconds.\n","-- Epoch 72\n","Norm: 0.39, NNZs: 3, Bias: -0.000010, T: 10800, Avg. loss: 0.549665\n","Total training time: 0.02 seconds.\n","-- Epoch 73\n","Norm: 0.40, NNZs: 3, Bias: -0.000010, T: 10950, Avg. loss: 0.548147\n","Total training time: 0.02 seconds.\n","-- Epoch 74\n","Norm: 0.40, NNZs: 3, Bias: -0.000011, T: 11100, Avg. loss: 0.546640\n","Total training time: 0.02 seconds.\n","-- Epoch 75\n","Norm: 0.41, NNZs: 3, Bias: -0.000012, T: 11250, Avg. loss: 0.545143\n","Total training time: 0.02 seconds.\n","-- Epoch 76\n","Norm: 0.41, NNZs: 3, Bias: -0.000012, T: 11400, Avg. loss: 0.543656\n","Total training time: 0.02 seconds.\n","-- Epoch 77\n","Norm: 0.42, NNZs: 3, Bias: -0.000013, T: 11550, Avg. loss: 0.542180\n","Total training time: 0.02 seconds.\n","-- Epoch 78\n","Norm: 0.42, NNZs: 3, Bias: -0.000014, T: 11700, Avg. loss: 0.540715\n","Total training time: 0.03 seconds.\n","-- Epoch 79\n","Norm: 0.43, NNZs: 3, Bias: -0.000014, T: 11850, Avg. loss: 0.539260\n","Total training time: 0.03 seconds.\n","-- Epoch 80\n","Norm: 0.43, NNZs: 3, Bias: -0.000015, T: 12000, Avg. loss: 0.537814\n","Total training time: 0.03 seconds.\n","-- Epoch 81\n","Norm: 0.44, NNZs: 3, Bias: -0.000015, T: 12150, Avg. loss: 0.536379\n","Total training time: 0.03 seconds.\n","-- Epoch 82\n","Norm: 0.44, NNZs: 3, Bias: -0.000016, T: 12300, Avg. loss: 0.534954\n","Total training time: 0.03 seconds.\n","-- Epoch 83\n","Norm: 0.44, NNZs: 3, Bias: -0.000017, T: 12450, Avg. loss: 0.533539\n","Total training time: 0.03 seconds.\n","-- Epoch 84\n","Norm: 0.45, NNZs: 3, Bias: -0.000018, T: 12600, Avg. loss: 0.532134\n","Total training time: 0.03 seconds.\n","-- Epoch 85\n","Norm: 0.45, NNZs: 3, Bias: -0.000019, T: 12750, Avg. loss: 0.530738\n","Total training time: 0.03 seconds.\n","-- Epoch 86\n","Norm: 0.46, NNZs: 3, Bias: -0.000019, T: 12900, Avg. loss: 0.529353\n","Total training time: 0.03 seconds.\n","-- Epoch 87\n","Norm: 0.46, NNZs: 3, Bias: -0.000021, T: 13050, Avg. loss: 0.527976\n","Total training time: 0.03 seconds.\n","-- Epoch 88\n","Norm: 0.47, NNZs: 3, Bias: -0.000021, T: 13200, Avg. loss: 0.526609\n","Total training time: 0.03 seconds.\n","-- Epoch 89\n","Norm: 0.47, NNZs: 3, Bias: -0.000022, T: 13350, Avg. loss: 0.525252\n","Total training time: 0.03 seconds.\n","-- Epoch 90\n","Norm: 0.48, NNZs: 3, Bias: -0.000023, T: 13500, Avg. loss: 0.523904\n","Total training time: 0.03 seconds.\n","-- Epoch 91\n","Norm: 0.48, NNZs: 3, Bias: -0.000024, T: 13650, Avg. loss: 0.522564\n","Total training time: 0.03 seconds.\n","-- Epoch 92\n","Norm: 0.49, NNZs: 3, Bias: -0.000025, T: 13800, Avg. loss: 0.521235\n","Total training time: 0.03 seconds.\n","-- Epoch 93\n","Norm: 0.49, NNZs: 3, Bias: -0.000024, T: 13950, Avg. loss: 0.519914\n","Total training time: 0.03 seconds.\n","-- Epoch 94\n","Norm: 0.49, NNZs: 3, Bias: -0.000026, T: 14100, Avg. loss: 0.518602\n","Total training time: 0.03 seconds.\n","-- Epoch 95\n","Norm: 0.50, NNZs: 3, Bias: -0.000026, T: 14250, Avg. loss: 0.517299\n","Total training time: 0.03 seconds.\n","-- Epoch 96\n","Norm: 0.50, NNZs: 3, Bias: -0.000027, T: 14400, Avg. loss: 0.516005\n","Total training time: 0.03 seconds.\n","-- Epoch 97\n","Norm: 0.51, NNZs: 3, Bias: -0.000029, T: 14550, Avg. loss: 0.514720\n","Total training time: 0.03 seconds.\n","-- Epoch 98\n","Norm: 0.51, NNZs: 3, Bias: -0.000029, T: 14700, Avg. loss: 0.513443\n","Total training time: 0.03 seconds.\n","-- Epoch 99\n","Norm: 0.52, NNZs: 3, Bias: -0.000029, T: 14850, Avg. loss: 0.512175\n","Total training time: 0.03 seconds.\n","-- Epoch 100\n","Norm: 0.52, NNZs: 3, Bias: -0.000030, T: 15000, Avg. loss: 0.510915\n","Total training time: 0.03 seconds.\n","-- Epoch 101\n","Norm: 0.53, NNZs: 3, Bias: -0.000031, T: 15150, Avg. loss: 0.509664\n","Total training time: 0.03 seconds.\n","-- Epoch 102\n","Norm: 0.53, NNZs: 3, Bias: -0.000032, T: 15300, Avg. loss: 0.508422\n","Total training time: 0.03 seconds.\n","-- Epoch 103\n","Norm: 0.53, NNZs: 3, Bias: -0.000033, T: 15450, Avg. loss: 0.507187\n","Total training time: 0.03 seconds.\n","-- Epoch 104\n","Norm: 0.54, NNZs: 3, Bias: -0.000035, T: 15600, Avg. loss: 0.505961\n","Total training time: 0.03 seconds.\n","-- Epoch 105\n","Norm: 0.54, NNZs: 3, Bias: -0.000036, T: 15750, Avg. loss: 0.504743\n","Total training time: 0.03 seconds.\n","-- Epoch 106\n","Norm: 0.55, NNZs: 3, Bias: -0.000037, T: 15900, Avg. loss: 0.503533\n","Total training time: 0.03 seconds.\n","-- Epoch 107\n","Norm: 0.55, NNZs: 3, Bias: -0.000039, T: 16050, Avg. loss: 0.502331\n","Total training time: 0.03 seconds.\n","-- Epoch 108\n","Norm: 0.55, NNZs: 3, Bias: -0.000039, T: 16200, Avg. loss: 0.501137\n","Total training time: 0.03 seconds.\n","-- Epoch 109\n","Norm: 0.56, NNZs: 3, Bias: -0.000040, T: 16350, Avg. loss: 0.499950\n","Total training time: 0.03 seconds.\n","-- Epoch 110\n","Norm: 0.56, NNZs: 3, Bias: -0.000041, T: 16500, Avg. loss: 0.498772\n","Total training time: 0.03 seconds.\n","-- Epoch 111\n","Norm: 0.57, NNZs: 3, Bias: -0.000043, T: 16650, Avg. loss: 0.497601\n","Total training time: 0.03 seconds.\n","-- Epoch 112\n","Norm: 0.57, NNZs: 3, Bias: -0.000044, T: 16800, Avg. loss: 0.496438\n","Total training time: 0.03 seconds.\n","-- Epoch 113\n","Norm: 0.58, NNZs: 3, Bias: -0.000045, T: 16950, Avg. loss: 0.495283\n","Total training time: 0.03 seconds.\n","-- Epoch 114\n","Norm: 0.58, NNZs: 3, Bias: -0.000046, T: 17100, Avg. loss: 0.494135\n","Total training time: 0.03 seconds.\n","-- Epoch 115\n","Norm: 0.58, NNZs: 3, Bias: -0.000047, T: 17250, Avg. loss: 0.492995\n","Total training time: 0.03 seconds.\n","-- Epoch 116\n","Norm: 0.59, NNZs: 3, Bias: -0.000049, T: 17400, Avg. loss: 0.491862\n","Total training time: 0.03 seconds.\n","-- Epoch 117\n","Norm: 0.59, NNZs: 3, Bias: -0.000050, T: 17550, Avg. loss: 0.490736\n","Total training time: 0.03 seconds.\n","-- Epoch 118\n","Norm: 0.60, NNZs: 3, Bias: -0.000052, T: 17700, Avg. loss: 0.489618\n","Total training time: 0.03 seconds.\n","-- Epoch 119\n","Norm: 0.60, NNZs: 3, Bias: -0.000054, T: 17850, Avg. loss: 0.488507\n","Total training time: 0.03 seconds.\n","-- Epoch 120\n","Norm: 0.60, NNZs: 3, Bias: -0.000055, T: 18000, Avg. loss: 0.487403\n","Total training time: 0.03 seconds.\n","-- Epoch 121\n","Norm: 0.61, NNZs: 3, Bias: -0.000057, T: 18150, Avg. loss: 0.486306\n","Total training time: 0.03 seconds.\n","-- Epoch 122\n","Norm: 0.61, NNZs: 3, Bias: -0.000058, T: 18300, Avg. loss: 0.485216\n","Total training time: 0.03 seconds.\n","-- Epoch 123\n","Norm: 0.62, NNZs: 3, Bias: -0.000059, T: 18450, Avg. loss: 0.484133\n","Total training time: 0.03 seconds.\n","-- Epoch 124\n","Norm: 0.62, NNZs: 3, Bias: -0.000060, T: 18600, Avg. loss: 0.483057\n","Total training time: 0.03 seconds.\n","-- Epoch 125\n","Norm: 0.62, NNZs: 3, Bias: -0.000061, T: 18750, Avg. loss: 0.481988\n","Total training time: 0.03 seconds.\n","-- Epoch 126\n","Norm: 0.63, NNZs: 3, Bias: -0.000063, T: 18900, Avg. loss: 0.480926\n","Total training time: 0.03 seconds.\n","-- Epoch 127\n","Norm: 0.63, NNZs: 3, Bias: -0.000064, T: 19050, Avg. loss: 0.479870\n","Total training time: 0.03 seconds.\n","-- Epoch 128\n","Norm: 0.64, NNZs: 3, Bias: -0.000065, T: 19200, Avg. loss: 0.478821\n","Total training time: 0.03 seconds.\n","-- Epoch 129\n","Norm: 0.64, NNZs: 3, Bias: -0.000068, T: 19350, Avg. loss: 0.477779\n","Total training time: 0.03 seconds.\n","-- Epoch 130\n","Norm: 0.64, NNZs: 3, Bias: -0.000069, T: 19500, Avg. loss: 0.476743\n","Total training time: 0.03 seconds.\n","-- Epoch 131\n","Norm: 0.65, NNZs: 3, Bias: -0.000072, T: 19650, Avg. loss: 0.475714\n","Total training time: 0.03 seconds.\n","-- Epoch 132\n","Norm: 0.65, NNZs: 3, Bias: -0.000074, T: 19800, Avg. loss: 0.474691\n","Total training time: 0.03 seconds.\n","-- Epoch 133\n","Norm: 0.66, NNZs: 3, Bias: -0.000075, T: 19950, Avg. loss: 0.473674\n","Total training time: 0.04 seconds.\n","-- Epoch 134\n","Norm: 0.66, NNZs: 3, Bias: -0.000077, T: 20100, Avg. loss: 0.472664\n","Total training time: 0.04 seconds.\n","-- Epoch 135\n","Norm: 0.66, NNZs: 3, Bias: -0.000079, T: 20250, Avg. loss: 0.471661\n","Total training time: 0.04 seconds.\n","-- Epoch 136\n","Norm: 0.67, NNZs: 3, Bias: -0.000081, T: 20400, Avg. loss: 0.470663\n","Total training time: 0.04 seconds.\n","-- Epoch 137\n","Norm: 0.67, NNZs: 3, Bias: -0.000084, T: 20550, Avg. loss: 0.469672\n","Total training time: 0.04 seconds.\n","-- Epoch 138\n","Norm: 0.68, NNZs: 3, Bias: -0.000085, T: 20700, Avg. loss: 0.468687\n","Total training time: 0.04 seconds.\n","-- Epoch 139\n","Norm: 0.68, NNZs: 3, Bias: -0.000087, T: 20850, Avg. loss: 0.467707\n","Total training time: 0.04 seconds.\n","-- Epoch 140\n","Norm: 0.68, NNZs: 3, Bias: -0.000088, T: 21000, Avg. loss: 0.466734\n","Total training time: 0.04 seconds.\n","Convergence after 140 epochs took 0.04 seconds\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["array([[0.05514966, 0.00320732, 0.68086063]])"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"hPZsyzIi2F3t","colab_type":"code","colab":{},"outputId":"75b4e551-07f9-4198-98ee-5e55497706d0"},"source":["from sklearn import preprocessing\n","from sklearn.model_selection import train_test_split\n","from sklearn import linear_model\n","X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.25, random_state=15)\n","clf = linear_model.SGDClassifier(eta0=0.0001, alpha=0.0001, loss='hinge', random_state=15, penalty='l2', tol=1e-3, verbose=2, learning_rate='constant')\n","scaler = StandardScaler()\n","X_std = scaler.fit_transform(X_train)\n","#Y_std = scaler.fit_transform( y_train)\n","clf.fit(X=X_std, y=y_train)\n","clf.coef_\n","#since feature f1 and f2 as high variance compared to f3 after standarization its weight reduced compared to  feature f3\n","#i.e.,var(F2)>>var(F1)>>Var(F3) implie Weight f2<weight f1<weight f3 after stanradization"],"execution_count":0,"outputs":[{"output_type":"stream","text":["-- Epoch 1\n","Norm: 0.01, NNZs: 3, Bias: 0.000000, T: 150, Avg. loss: 0.994997\n","Total training time: 0.00 seconds.\n","-- Epoch 2\n","Norm: 0.02, NNZs: 3, Bias: -0.000000, T: 300, Avg. loss: 0.984591\n","Total training time: 0.00 seconds.\n","-- Epoch 3\n","Norm: 0.04, NNZs: 3, Bias: -0.000000, T: 450, Avg. loss: 0.974185\n","Total training time: 0.00 seconds.\n","-- Epoch 4\n","Norm: 0.05, NNZs: 3, Bias: 0.000000, T: 600, Avg. loss: 0.963779\n","Total training time: 0.00 seconds.\n","-- Epoch 5\n","Norm: 0.06, NNZs: 3, Bias: 0.000000, T: 750, Avg. loss: 0.953374\n","Total training time: 0.00 seconds.\n","-- Epoch 6\n","Norm: 0.07, NNZs: 3, Bias: 0.000000, T: 900, Avg. loss: 0.942968\n","Total training time: 0.00 seconds.\n","-- Epoch 7\n","Norm: 0.09, NNZs: 3, Bias: 0.000000, T: 1050, Avg. loss: 0.932562\n","Total training time: 0.00 seconds.\n","-- Epoch 8\n","Norm: 0.10, NNZs: 3, Bias: -0.000000, T: 1200, Avg. loss: 0.922156\n","Total training time: 0.00 seconds.\n","-- Epoch 9\n","Norm: 0.11, NNZs: 3, Bias: -0.000000, T: 1350, Avg. loss: 0.911750\n","Total training time: 0.01 seconds.\n","-- Epoch 10\n","Norm: 0.12, NNZs: 3, Bias: 0.000000, T: 1500, Avg. loss: 0.901345\n","Total training time: 0.01 seconds.\n","-- Epoch 11\n","Norm: 0.14, NNZs: 3, Bias: 0.000000, T: 1650, Avg. loss: 0.890939\n","Total training time: 0.01 seconds.\n","-- Epoch 12\n","Norm: 0.15, NNZs: 3, Bias: -0.000000, T: 1800, Avg. loss: 0.880533\n","Total training time: 0.01 seconds.\n","-- Epoch 13\n","Norm: 0.16, NNZs: 3, Bias: -0.000000, T: 1950, Avg. loss: 0.870127\n","Total training time: 0.01 seconds.\n","-- Epoch 14\n","Norm: 0.17, NNZs: 3, Bias: -0.000000, T: 2100, Avg. loss: 0.859722\n","Total training time: 0.01 seconds.\n","-- Epoch 15\n","Norm: 0.19, NNZs: 3, Bias: -0.000000, T: 2250, Avg. loss: 0.849316\n","Total training time: 0.01 seconds.\n","-- Epoch 16\n","Norm: 0.20, NNZs: 3, Bias: 0.000000, T: 2400, Avg. loss: 0.838910\n","Total training time: 0.01 seconds.\n","-- Epoch 17\n","Norm: 0.21, NNZs: 3, Bias: 0.000000, T: 2550, Avg. loss: 0.828505\n","Total training time: 0.01 seconds.\n","-- Epoch 18\n","Norm: 0.22, NNZs: 3, Bias: 0.000000, T: 2700, Avg. loss: 0.818099\n","Total training time: 0.01 seconds.\n","-- Epoch 19\n","Norm: 0.24, NNZs: 3, Bias: 0.000000, T: 2850, Avg. loss: 0.807693\n","Total training time: 0.01 seconds.\n","-- Epoch 20\n","Norm: 0.25, NNZs: 3, Bias: 0.000000, T: 3000, Avg. loss: 0.797288\n","Total training time: 0.01 seconds.\n","-- Epoch 21\n","Norm: 0.26, NNZs: 3, Bias: 0.000000, T: 3150, Avg. loss: 0.786882\n","Total training time: 0.01 seconds.\n","-- Epoch 22\n","Norm: 0.27, NNZs: 3, Bias: -0.000000, T: 3300, Avg. loss: 0.776476\n","Total training time: 0.01 seconds.\n","-- Epoch 23\n","Norm: 0.29, NNZs: 3, Bias: 0.000000, T: 3450, Avg. loss: 0.766071\n","Total training time: 0.01 seconds.\n","-- Epoch 24\n","Norm: 0.30, NNZs: 3, Bias: -0.000000, T: 3600, Avg. loss: 0.755665\n","Total training time: 0.01 seconds.\n","-- Epoch 25\n","Norm: 0.31, NNZs: 3, Bias: 0.000000, T: 3750, Avg. loss: 0.745260\n","Total training time: 0.01 seconds.\n","-- Epoch 26\n","Norm: 0.32, NNZs: 3, Bias: 0.000000, T: 3900, Avg. loss: 0.734854\n","Total training time: 0.01 seconds.\n","-- Epoch 27\n","Norm: 0.34, NNZs: 3, Bias: 0.000000, T: 4050, Avg. loss: 0.724449\n","Total training time: 0.01 seconds.\n","-- Epoch 28\n","Norm: 0.35, NNZs: 3, Bias: -0.000000, T: 4200, Avg. loss: 0.714043\n","Total training time: 0.01 seconds.\n","-- Epoch 29\n","Norm: 0.36, NNZs: 3, Bias: 0.000000, T: 4350, Avg. loss: 0.703638\n","Total training time: 0.01 seconds.\n","-- Epoch 30\n","Norm: 0.37, NNZs: 3, Bias: 0.000000, T: 4500, Avg. loss: 0.693232\n","Total training time: 0.01 seconds.\n","-- Epoch 31\n","Norm: 0.39, NNZs: 3, Bias: -0.000000, T: 4650, Avg. loss: 0.682827\n","Total training time: 0.01 seconds.\n","-- Epoch 32\n","Norm: 0.40, NNZs: 3, Bias: 0.000000, T: 4800, Avg. loss: 0.672421\n","Total training time: 0.01 seconds.\n","-- Epoch 33\n","Norm: 0.41, NNZs: 3, Bias: 0.000000, T: 4950, Avg. loss: 0.662016\n","Total training time: 0.01 seconds.\n","-- Epoch 34\n","Norm: 0.42, NNZs: 3, Bias: -0.000000, T: 5100, Avg. loss: 0.651611\n","Total training time: 0.01 seconds.\n","-- Epoch 35\n","Norm: 0.44, NNZs: 3, Bias: 0.000000, T: 5250, Avg. loss: 0.641205\n","Total training time: 0.01 seconds.\n","-- Epoch 36\n","Norm: 0.45, NNZs: 3, Bias: -0.000000, T: 5400, Avg. loss: 0.630800\n","Total training time: 0.01 seconds.\n","-- Epoch 37\n","Norm: 0.46, NNZs: 3, Bias: -0.000000, T: 5550, Avg. loss: 0.620395\n","Total training time: 0.01 seconds.\n","-- Epoch 38\n","Norm: 0.47, NNZs: 3, Bias: -0.000000, T: 5700, Avg. loss: 0.609989\n","Total training time: 0.01 seconds.\n","-- Epoch 39\n","Norm: 0.49, NNZs: 3, Bias: -0.000000, T: 5850, Avg. loss: 0.599584\n","Total training time: 0.01 seconds.\n","-- Epoch 40\n","Norm: 0.50, NNZs: 3, Bias: -0.000000, T: 6000, Avg. loss: 0.589179\n","Total training time: 0.02 seconds.\n","-- Epoch 41\n","Norm: 0.51, NNZs: 3, Bias: 0.000000, T: 6150, Avg. loss: 0.578773\n","Total training time: 0.02 seconds.\n","-- Epoch 42\n","Norm: 0.52, NNZs: 3, Bias: -0.000000, T: 6300, Avg. loss: 0.568368\n","Total training time: 0.02 seconds.\n","-- Epoch 43\n","Norm: 0.54, NNZs: 3, Bias: -0.000100, T: 6450, Avg. loss: 0.558045\n","Total training time: 0.02 seconds.\n","-- Epoch 44\n","Norm: 0.55, NNZs: 3, Bias: -0.000200, T: 6600, Avg. loss: 0.547949\n","Total training time: 0.02 seconds.\n","-- Epoch 45\n","Norm: 0.56, NNZs: 3, Bias: -0.000200, T: 6750, Avg. loss: 0.537931\n","Total training time: 0.02 seconds.\n","-- Epoch 46\n","Norm: 0.57, NNZs: 3, Bias: -0.000100, T: 6900, Avg. loss: 0.528694\n","Total training time: 0.02 seconds.\n","-- Epoch 47\n","Norm: 0.58, NNZs: 3, Bias: 0.000100, T: 7050, Avg. loss: 0.519843\n","Total training time: 0.02 seconds.\n","-- Epoch 48\n","Norm: 0.60, NNZs: 3, Bias: 0.000400, T: 7200, Avg. loss: 0.511514\n","Total training time: 0.02 seconds.\n","-- Epoch 49\n","Norm: 0.61, NNZs: 3, Bias: 0.000700, T: 7350, Avg. loss: 0.503804\n","Total training time: 0.02 seconds.\n","-- Epoch 50\n","Norm: 0.62, NNZs: 3, Bias: 0.001000, T: 7500, Avg. loss: 0.496477\n","Total training time: 0.02 seconds.\n","-- Epoch 51\n","Norm: 0.63, NNZs: 3, Bias: 0.001200, T: 7650, Avg. loss: 0.489554\n","Total training time: 0.02 seconds.\n","-- Epoch 52\n","Norm: 0.64, NNZs: 3, Bias: 0.001500, T: 7800, Avg. loss: 0.482810\n","Total training time: 0.02 seconds.\n","-- Epoch 53\n","Norm: 0.65, NNZs: 3, Bias: 0.001800, T: 7950, Avg. loss: 0.476168\n","Total training time: 0.02 seconds.\n","-- Epoch 54\n","Norm: 0.66, NNZs: 3, Bias: 0.001800, T: 8100, Avg. loss: 0.469733\n","Total training time: 0.02 seconds.\n","-- Epoch 55\n","Norm: 0.66, NNZs: 3, Bias: 0.001700, T: 8250, Avg. loss: 0.463772\n","Total training time: 0.02 seconds.\n","-- Epoch 56\n","Norm: 0.67, NNZs: 3, Bias: 0.001400, T: 8400, Avg. loss: 0.458291\n","Total training time: 0.02 seconds.\n","-- Epoch 57\n","Norm: 0.68, NNZs: 3, Bias: 0.001000, T: 8550, Avg. loss: 0.453242\n","Total training time: 0.02 seconds.\n","-- Epoch 58\n","Norm: 0.69, NNZs: 3, Bias: 0.000500, T: 8700, Avg. loss: 0.448353\n","Total training time: 0.02 seconds.\n","-- Epoch 59\n","Norm: 0.70, NNZs: 3, Bias: 0.000100, T: 8850, Avg. loss: 0.443662\n","Total training time: 0.02 seconds.\n","-- Epoch 60\n","Norm: 0.71, NNZs: 3, Bias: -0.000400, T: 9000, Avg. loss: 0.439108\n","Total training time: 0.02 seconds.\n","-- Epoch 61\n","Norm: 0.71, NNZs: 3, Bias: -0.000700, T: 9150, Avg. loss: 0.434750\n","Total training time: 0.02 seconds.\n","-- Epoch 62\n","Norm: 0.72, NNZs: 3, Bias: -0.000800, T: 9300, Avg. loss: 0.430749\n","Total training time: 0.02 seconds.\n","-- Epoch 63\n","Norm: 0.73, NNZs: 3, Bias: -0.000800, T: 9450, Avg. loss: 0.426919\n","Total training time: 0.02 seconds.\n","-- Epoch 64\n","Norm: 0.74, NNZs: 3, Bias: -0.000900, T: 9600, Avg. loss: 0.423228\n","Total training time: 0.02 seconds.\n","-- Epoch 65\n","Norm: 0.74, NNZs: 3, Bias: -0.001000, T: 9750, Avg. loss: 0.419570\n","Total training time: 0.02 seconds.\n","-- Epoch 66\n","Norm: 0.75, NNZs: 3, Bias: -0.001000, T: 9900, Avg. loss: 0.416024\n","Total training time: 0.02 seconds.\n","-- Epoch 67\n","Norm: 0.76, NNZs: 3, Bias: -0.001100, T: 10050, Avg. loss: 0.412565\n","Total training time: 0.02 seconds.\n","-- Epoch 68\n","Norm: 0.77, NNZs: 3, Bias: -0.001100, T: 10200, Avg. loss: 0.409186\n","Total training time: 0.02 seconds.\n","-- Epoch 69\n","Norm: 0.77, NNZs: 3, Bias: -0.001000, T: 10350, Avg. loss: 0.405930\n","Total training time: 0.03 seconds.\n","-- Epoch 70\n","Norm: 0.78, NNZs: 3, Bias: -0.000800, T: 10500, Avg. loss: 0.402789\n","Total training time: 0.03 seconds.\n","-- Epoch 71\n","Norm: 0.79, NNZs: 3, Bias: -0.000500, T: 10650, Avg. loss: 0.399761\n","Total training time: 0.03 seconds.\n","-- Epoch 72\n","Norm: 0.79, NNZs: 3, Bias: -0.000300, T: 10800, Avg. loss: 0.396823\n","Total training time: 0.03 seconds.\n","-- Epoch 73\n","Norm: 0.80, NNZs: 3, Bias: -0.000000, T: 10950, Avg. loss: 0.394029\n","Total training time: 0.03 seconds.\n","-- Epoch 74\n","Norm: 0.80, NNZs: 3, Bias: 0.000600, T: 11100, Avg. loss: 0.391403\n","Total training time: 0.03 seconds.\n","-- Epoch 75\n","Norm: 0.81, NNZs: 3, Bias: 0.001300, T: 11250, Avg. loss: 0.388990\n","Total training time: 0.03 seconds.\n","-- Epoch 76\n","Norm: 0.82, NNZs: 3, Bias: 0.001800, T: 11400, Avg. loss: 0.386723\n","Total training time: 0.03 seconds.\n","-- Epoch 77\n","Norm: 0.82, NNZs: 3, Bias: 0.002300, T: 11550, Avg. loss: 0.384589\n","Total training time: 0.03 seconds.\n","-- Epoch 78\n","Norm: 0.83, NNZs: 3, Bias: 0.002700, T: 11700, Avg. loss: 0.382505\n","Total training time: 0.03 seconds.\n","-- Epoch 79\n","Norm: 0.83, NNZs: 3, Bias: 0.003100, T: 11850, Avg. loss: 0.380481\n","Total training time: 0.03 seconds.\n","-- Epoch 80\n","Norm: 0.84, NNZs: 3, Bias: 0.003500, T: 12000, Avg. loss: 0.378457\n","Total training time: 0.03 seconds.\n","-- Epoch 81\n","Norm: 0.84, NNZs: 3, Bias: 0.003900, T: 12150, Avg. loss: 0.376432\n","Total training time: 0.03 seconds.\n","-- Epoch 82\n","Norm: 0.85, NNZs: 3, Bias: 0.004300, T: 12300, Avg. loss: 0.374408\n","Total training time: 0.03 seconds.\n","-- Epoch 83\n","Norm: 0.85, NNZs: 3, Bias: 0.004600, T: 12450, Avg. loss: 0.372432\n","Total training time: 0.03 seconds.\n","-- Epoch 84\n","Norm: 0.86, NNZs: 3, Bias: 0.004900, T: 12600, Avg. loss: 0.370499\n","Total training time: 0.03 seconds.\n","-- Epoch 85\n","Norm: 0.86, NNZs: 3, Bias: 0.005200, T: 12750, Avg. loss: 0.368566\n","Total training time: 0.03 seconds.\n","-- Epoch 86\n","Norm: 0.87, NNZs: 3, Bias: 0.005500, T: 12900, Avg. loss: 0.366633\n","Total training time: 0.03 seconds.\n","-- Epoch 87\n","Norm: 0.87, NNZs: 3, Bias: 0.005700, T: 13050, Avg. loss: 0.364732\n","Total training time: 0.03 seconds.\n","-- Epoch 88\n","Norm: 0.88, NNZs: 3, Bias: 0.005900, T: 13200, Avg. loss: 0.362881\n","Total training time: 0.03 seconds.\n","-- Epoch 89\n","Norm: 0.88, NNZs: 3, Bias: 0.006100, T: 13350, Avg. loss: 0.361031\n","Total training time: 0.03 seconds.\n","-- Epoch 90\n","Norm: 0.89, NNZs: 3, Bias: 0.006300, T: 13500, Avg. loss: 0.359180\n","Total training time: 0.03 seconds.\n","-- Epoch 91\n","Norm: 0.90, NNZs: 3, Bias: 0.006500, T: 13650, Avg. loss: 0.357330\n","Total training time: 0.03 seconds.\n","-- Epoch 92\n","Norm: 0.90, NNZs: 3, Bias: 0.006700, T: 13800, Avg. loss: 0.355480\n","Total training time: 0.03 seconds.\n","-- Epoch 93\n","Norm: 0.91, NNZs: 3, Bias: 0.006900, T: 13950, Avg. loss: 0.353629\n","Total training time: 0.03 seconds.\n","-- Epoch 94\n","Norm: 0.91, NNZs: 3, Bias: 0.007000, T: 14100, Avg. loss: 0.351806\n","Total training time: 0.03 seconds.\n","-- Epoch 95\n","Norm: 0.92, NNZs: 3, Bias: 0.007100, T: 14250, Avg. loss: 0.350022\n","Total training time: 0.03 seconds.\n","-- Epoch 96\n","Norm: 0.92, NNZs: 3, Bias: 0.007000, T: 14400, Avg. loss: 0.348310\n","Total training time: 0.03 seconds.\n","-- Epoch 97\n","Norm: 0.93, NNZs: 3, Bias: 0.007000, T: 14550, Avg. loss: 0.346687\n","Total training time: 0.03 seconds.\n","-- Epoch 98\n","Norm: 0.93, NNZs: 3, Bias: 0.007100, T: 14700, Avg. loss: 0.345174\n","Total training time: 0.03 seconds.\n","-- Epoch 99\n","Norm: 0.93, NNZs: 3, Bias: 0.007100, T: 14850, Avg. loss: 0.343697\n","Total training time: 0.03 seconds.\n","-- Epoch 100\n","Norm: 0.94, NNZs: 3, Bias: 0.007100, T: 15000, Avg. loss: 0.342270\n","Total training time: 0.03 seconds.\n","-- Epoch 101\n","Norm: 0.94, NNZs: 3, Bias: 0.006900, T: 15150, Avg. loss: 0.340869\n","Total training time: 0.03 seconds.\n","-- Epoch 102\n","Norm: 0.95, NNZs: 3, Bias: 0.006700, T: 15300, Avg. loss: 0.339556\n","Total training time: 0.03 seconds.\n","-- Epoch 103\n","Norm: 0.95, NNZs: 3, Bias: 0.006500, T: 15450, Avg. loss: 0.338242\n","Total training time: 0.03 seconds.\n","-- Epoch 104\n","Norm: 0.96, NNZs: 3, Bias: 0.006400, T: 15600, Avg. loss: 0.336965\n","Total training time: 0.03 seconds.\n","-- Epoch 105\n","Norm: 0.96, NNZs: 3, Bias: 0.006300, T: 15750, Avg. loss: 0.335696\n","Total training time: 0.03 seconds.\n","-- Epoch 106\n","Norm: 0.96, NNZs: 3, Bias: 0.006400, T: 15900, Avg. loss: 0.334524\n","Total training time: 0.03 seconds.\n","-- Epoch 107\n","Norm: 0.97, NNZs: 3, Bias: 0.006500, T: 16050, Avg. loss: 0.333467\n","Total training time: 0.03 seconds.\n","-- Epoch 108\n","Norm: 0.97, NNZs: 3, Bias: 0.006600, T: 16200, Avg. loss: 0.332446\n","Total training time: 0.03 seconds.\n","-- Epoch 109\n","Norm: 0.98, NNZs: 3, Bias: 0.006700, T: 16350, Avg. loss: 0.331424\n","Total training time: 0.03 seconds.\n","-- Epoch 110\n","Norm: 0.98, NNZs: 3, Bias: 0.006700, T: 16500, Avg. loss: 0.330424\n","Total training time: 0.03 seconds.\n","-- Epoch 111\n","Norm: 0.98, NNZs: 3, Bias: 0.006700, T: 16650, Avg. loss: 0.329440\n","Total training time: 0.03 seconds.\n","-- Epoch 112\n","Norm: 0.99, NNZs: 3, Bias: 0.006600, T: 16800, Avg. loss: 0.328494\n","Total training time: 0.03 seconds.\n","-- Epoch 113\n","Norm: 0.99, NNZs: 3, Bias: 0.006500, T: 16950, Avg. loss: 0.327573\n","Total training time: 0.03 seconds.\n","-- Epoch 114\n","Norm: 0.99, NNZs: 3, Bias: 0.006300, T: 17100, Avg. loss: 0.326668\n","Total training time: 0.03 seconds.\n","-- Epoch 115\n","Norm: 1.00, NNZs: 3, Bias: 0.006100, T: 17250, Avg. loss: 0.325797\n","Total training time: 0.03 seconds.\n","Convergence after 115 epochs took 0.03 seconds\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["array([[0.0705479 , 0.05549049, 0.99391018]])"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"_SMq9u8B2F3w","colab_type":"text"},"source":[""]}]}